# Depth Any Video with Scalable Synthetic Data

**Depth Any Video** introduces a **scalable synthetic data** pipeline, capturing **40,000** video clips from diverse game environments, and leverages generative **video diffusion models** to advance video depth estimation. By incorporating rotary position encoding, flow matching, and a mixed-duration training strategy, it robustly handles **varying video lengths and frame rates**. Additionally, a novel depth interpolation method enables **high-resolution depth inference**, achieving superior spatial accuracy and temporal consistency compared to previous models.

This repository is the official implementation of the paper:
<div align='center'>

**Depth Any Video with Scalable Synthetic Data**

[*Honghui Yang**](https://hhyangcs.github.io/),
[*Di Huang**](https://dihuang.me/),
[*Wei Yin*](https://scholar.google.com/citations?user=ZIf_rtcAAAAJ),
[*Chunhua Shen*](https://scholar.google.com/citations?user=Ljk2BvIAAAAJ),
[*Haifeng Liu*](https://scholar.google.com/citations?user=oW108fUAAAAJ),
[*Xiaofei He*](https://scholar.google.com/citations?user=QLLFowsAAAAJ),
[*Binbin Lin+*](https://scholar.google.com/citations?user=Zmvq4KYAAAAJ),
[*Wanli Ouyang*](https://scholar.google.com/citations?user=pw_0Z_UAAAAJ),
[*Tong He+*](https://scholar.google.com/citations?user=kWADCMUAAAAJ)

[**[Project Page]**](https://depthanyvideo.github.io/) [**[Paper]**](?) [**[Video]**](?)
</div>

![teaser](assets/teaser.png)

## Citation

If you find our work useful, please cite:

```bibtex
@article{yang2024depthanyvideo,
  author    = {Honghui Yang and Di Huang and Wei Yin and Chunhua Shen and Haifeng Liu and Xiaofei He and Binbin Lin and Wanli Ouyang and Tong He},
  title     = {Depth Any Video with Scalable Synthetic Data},
  journal   = {arXiv preprint arxiv:?},
  year      = {2024}
}
```
